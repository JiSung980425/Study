크롤링 
    - 웹 페이지의 하이퍼링크를 순회하면서 웹 페이지를 다운로드하는 작업

스크레이핑
    - 다운로드한 웹 페이지에서 정보를 추출하는 작업

파이썬을 사용할 때의 장점
    - 언어 자체의 특성
    - 강력한 라이브러리
    - 스크레이핑 후처리의 편리성

가상환경
    - 런타임과 라이브러리를 환경(용도)에 따라 구분할 수 있는 기능
    - 버전 번호가 붙은 명령어를 사용하지 않더라도 python 명령어로 가상 환경 전용 파이썬 런타임을 실행 할 수 있다.

. 명령어
    - 매개변수로 지정한 파일을 읽어 들이고, 현재 셸에서 실행하는 명령어이다.

which 명령어
    - python 명령어의 경로를 확인

deactivate 명렁어
    - 가상 환경을 벗어날 수 있다.

EX1)
    import sys # import 구문으로 sys 모듈을 읽어 들인다

    def greet(name): # def 구문으로 greet라는 함수를 정의한다
        print('Hello, {0}!'.format(name)) #함수의 내용을 정의한다
    if len(sys.argv) > 1: # sys.argv라는 매개변수를 사용해 조건이 참이라면
        name = sys.argv[1] 
        greet(name)
    else:               
        greet('world')  # 조건문이 거짓이라면 greet 함수 호출

open()
    ex) with open('index.html') as f:
            print(f.read())
    - open() 함수의 반환값은 변수 f에 할당되며 with 블록 내부에서 사용한다.
    - 블록을 벗어날 때 f.close()가 자동으로 호출된다.

def 구문 복습
    def add(a,b): # add라는 이름의 함수를 정의하고 a,b 라는 매개변수를 받는다
        return a + b # add 함수는 a+b의 값을 반환한다
    print(add(1,2))

class 구문 복습
    class Rect: # Rect라는 이름의 클래스를 지정한다
        def __init__(self, width, height): # 인스턴스가 생성될때 호출되는 특수한 메서드를 정의한다
            self.width = width # width 속성에 값을 할당한다.
            self.height = height # height 속성에 값을 할당한다.
        def area(self): # 사각형의 넓이를 계산하는 메서드를 정의한다
            return self.width * self.height # 값 반환
    r = Rect(100, 200)
    print(r.width, r.height, r.area()) # 100 200 20000 출력

    class Square(Rect): # Rect를 상속받아 Square 클래스를 정의합니다.
        def __init__(self, width): # 부모 클래스의 메서드를 호출한다.
            super().__init__(width, width)

urllib으로 웹 페이지 추출하기
    from urllib.request import urlopen # urllib.request 라는 표준 라이버르러리 모듈을 사용해 포함되어있는 urlopen() 함수에 URL을 지정해 웹 페이지 추출가능
    f = urlopen('http://hanbit.co.kr') #
    print(type(f)) # urlopen() 함수는 HTTPResponse 자료형의 객체를 반환한다
    print(f.read()) # read() 메서드로 HTTP 응답 본문을 추출한다
    print(f.status) # 상태 코드를 추출한다

웹 페이지에서 데이터 추출하기
    - 정규 표현식으로 스크레이핑하기
        import re # re 모듈을 읽어 들입니다
        print(re.search(r'a*c', 'abc123DEF')) # re.search() 함수를 사용하면 두 번째 매개변수의 문자열이 첫 번째 매개변수의 정규 표현식에 맞는지 확인할 수 있다. 맞는 경우에 Match 객체를 반환하고 맞지 않으면 None을 반환한다
        print(re.search(r'a.*d', 'abc123DEF')) # 정규 표현식에 맞지않으므로 None 반환
        print(re.IGNORECASE) # 대소문자 무시
        m = re.search(r'a(.*)c', 'abc123DEF')
        print(m.group(0)) # group() 메서드로 일치한 값을 추출하는데 매개변수에 0을 지정하면 매치된 모든 값을 반환한다
        print(m.group(1)) # 매개변수에 1 이상의 숫자를 입력하면 정규 표현식에서 ()로 감싼 부분에 해당하는 값을 추출한다
        print(re.findall()) # 정규 표현식에 맞는 모든 부분을 추출
        print(re.sub()) # 정규 표현식에 맞는 부분을 바꿀 수 있다.

XML(RSS) 스크레이핑
    from xml.etree import ElementTree # ElementTree 모듈을 읽어 들인다
    tree = ElementTree.parse('rss.xml') # parse() 함수로 파일을 읽어 들이고 ElementTree 객체를 만든다
    root = tree.getroot() # getroot() 메서드로 XML의 루트 요소를 추출한다
    for item in root.findall('channel/item/description/body/location/dat'): # findall() 메서드로 요소 목록을 추출한다
        tm_ef = item.find('tmEF').text
        tmn = item.find('tmn').text
        tmx = item.find('tmx').text
        wf = item.find('wf').text
        print(tm_ef, tmn, tmx, wf)

파이썬으로 스크레이핑하는 흐름
    fetch(url) : 매개변수로 url을 받고 지정한 URL의 웹 페이지를 추출한다
    scrape(html) : 매개변수로 html을 받고, 정규 표현식을 사용해 HTML에서 도서 정보를 추출한다
    save(db_path, books) : 매개변수로 books라는 도서를 받고, SQLite 데이터베이스에 저장한다


    
