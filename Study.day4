Requests
    - "인간을 위한 HTTP"라는 캐리츠레이즈를 가지고 있는, 굉장히 사용하기 쉬운 라이브러리
    - 문자 코드 변환, 압축 등을 자동으로 처리해 주므로 낮는 레이러를 따로 의식하지 않고 사용가능
    ex)
        import Requests # 라이브러리를 읽어들인다
        r = Requests.get('http://hanbit.co.kr') # get() 함수로 웹 페이지를 추출한다
        type(r) # get() 함수의 반환값은 Response 자료형이다
        r.status_code # HTTP 상태 코드를 확인한다
        r.headers['content-type'] # headers 속성으로 HTTP 헤더를 딕셔너리로 추출한다
        r.encoding # encoding 속성으로 HTTP 헤더를 기반으로 인코딩을 추출한다
        r.text # text 속성으로 str 자료형으로 디코딩된 응답 본문을 추출한다
        r.content # content 속성으로 bytes 자료형의 응답 본문을 추출한다

Session
    - 여러 개의 페이지를 연속으로 크롤링할 때 사용하면 효과적
    - HTTP 헤더 또는 Basic 인증 등의 설정을 한 번만 하고 여러 번 재사용 가능
    ex)
        s = request.Session()
        s.headers.update({'user-agent': 'my-crawler/1.0 (+foo@example.com)'})

lmxl
    - C 언어로 작성된 XML처리와 관련된 유명한 라이브러리인 libxml2와 libxslt의 파이썬 바인딩
    - libxml2와 libxslt는 C 언어로 작성돼 있으므로 굉장히 빠르게 동작
    - C 언어 라이브러리를 래핑했다는 것 뿐만 아니라 파이썬에서 사용하기 쉽게 API를 구현
    - 기능이 너무 많아 처음 사용하는 사용자가 당황할 수 있다

Beautiful Soup
    - 간단하고 이해하기 쉬운 직관적인 API를 활용해 데이터를 추출 가능
    - 내부적으로 사용되는 파서를 목적에 맞게 변경 가능

pyquery
    - 자바스크립트 라이브러리인 jQuery와 같은 인터페이스로 스크레이핑할 수 있게 해준다
    - jQuer의 $ 함수에 css 선택자를 매개변수로 지정하는 것과 비슷한 형태로 사용하므로 jQuery에 익숙한 사용자라면 굉장히 쉽게 이해가능
    - pyquery는 내부적으로 lmxl을 사용

Beautiful Soup로 스크레이핑하기

    ex)
        from bs4 import BeautifulSoup # bs4 모듈에서 BeautifulSoup 클래스를 읽어 들인다
        # 첫 번째 매개변수에 파일 객체를 지정해 BeautifulSoup 객체를 생성
        # BeautifulSoup()에는 파일 이름 또는 URL을 지정할 수 없다
        # 두 번째 매개변수에는 파서의 종류를 지정한다
        with open('full_book_list.html') as f:
            soup = BeautifulSoup(f, 'html.parser')
        soup.h1 # soup.h1처럼 태그 이름을 가진 속성으로 h1 요소를 추출
        type(soup.h1) # 요소는 Tag 객체이다
        soup.h1.name # name 속성으로 태그 이름을 추출
        soup.h1.string # Tag 객체의 string 속성으로 요소 바로 아래의 문자열을 추출
        soup.ul.text # text 속성은 요소 내부의 모든 문자열을 결합해서 문자열을 추출
        soup.h1['id'] # Tag 객체는 딕셔너리처럼 속성을 추출 가능
        soup.find('li") # 여러 개의 요소가 있는 경우 가장 앞의 요소를 추출
        soup.find_all('li', calss_='featured') # find_all 메서드로 지정한 이름의 요소 리트를 추출

RSS 스크레이핑하기
    
    ex)
        import feedparser
        d = feedparser.parse('it.rss') # parse() 함수에 파일 경로, 파일 객체, XML 문자열 등을 지정
        d = feedparser.parse('www.aladin.co.kr/rss/new_all/351') # parse() 함수에 URL을 지정하면 피드를 파싱할 수 있다
        d.version # 피드의 버전을 추출
        d.feed.title # 피드의 타이틀 추출한다
        d.entries[0].title # 요소의 타이틀 추출
        d.entries[0].updated_parsed # 요소의 변경 시간을 파싱해서 time.struct_time 자료형으로 추출

URL 기초 기식
    - 크롤러로 링크를 돌아다니려면 링크를 나타내는 a 태그의 href 속성에서 다른 페이지의 URL을 추출해야 한다
    - 추출한 URL이 상대 URL이라면 절대 URL로 변환해야 한다
    URL 구성요소
        스키마 : http 또는 https와 같은 프로토콜
        어서리티 : // 뒤에 나온는 일반적인 호스트 이름
        경로 : /로 시작하는 해당 호스트 내부에서의 리소스 경로를 나타낸다
        쿼리 : ? 뒤에 나오는 경로와는 다른 방법으로 리소를 표현하는 방법
        플래그먼트 : # 뒤에 나오는 리소스 내부의 특정 부분 등을 나타낸다
